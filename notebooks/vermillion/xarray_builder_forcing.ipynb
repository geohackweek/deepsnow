{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to bring in and downsample the forcings\n",
    "The goal of this notebook is to load in the forcings data and to create an x.array that is similar in dimesnion and extent to the tolumne data.\n",
    "\n",
    "The varaibles for the x array are all of the primary forcings, and they are going to be concatinated based on time. \n",
    "\n",
    "The coordinate system needs to be transformed, the data needs to be subsetted by extent of the tolumne, and downsampled possibly?\n",
    "\n",
    "1. Load in the Forcings dataset into an X-array\n",
    "2. Get spatial extent from the ASO Data\n",
    "3. Transform the extent to Lat/Long\n",
    "4. Clip the spatial extent of the Forcings data\n",
    "5. Bin the data by the date of the ASO flights\n",
    "6. Apply the find freezing dates \n",
    "7. Sum the dates to get freezing dates\n",
    "8. Save the array to go into the CNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import gdal\n",
    "import osr\n",
    "import os\n",
    "import time\n",
    "import xarray as xr\n",
    "import pyproj \n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "# Define the path to all of the data you want to load\n",
    "path = '/srv/shared/deep_stac/data/forcings/2017/*2017.nc'\n",
    "# Make the path a global varaible and make a list of the files \n",
    "flist=glob.glob(path)\n",
    "print(len(flist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All of the forcings data is contained within a text file called \"forcings_key.txt\"\n",
    "\n",
    "This file can be used to cycle through each netCDF input and stitch it together in the xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /home/jovyan/ghw2019_deepstac/notebooks/vermillion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and filter to the 'primary' forcings \n",
    "pth = '/home/jovyan/ghw2019_deepstac/notebooks/vermillion'\n",
    "fname = 'forcings_key.txt'\n",
    "\n",
    "i = 1\n",
    "key = np.genfromtxt(os.path.join(pth,fname), dtype='str', delimiter=',')\n",
    "\n",
    "len(key)\n",
    "\n",
    "pared = key[key[:,1]=='primary']\n",
    "\n",
    "# And the names of the files are in:\n",
    "pared[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a function for renaming (on the fly) input datasets with duplicate names\n",
    "(for example, inside the netCDF objects, both rmax and rmin were assigned a name of \"relative_humidity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.basename('/srv/shared/deep_stac/data/forcings/2014/pr_2014.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load up the data into xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will need to filter the list based on ones we want - Eric's code  \n",
    "\n",
    "#ds=xr.open_mfdataset(flist,  concat_dim='day') #dataset\n",
    "# mf is multiple file dataset \n",
    "#ds.close()\n",
    "#ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workflow for one file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/xarray/conventions.py:448: SerializationWarning: variable 'air_temperature' has _Unsigned attribute but is not of integer type. Ignoring attribute.\n",
      "  stack_char_dim=stack_char_dim, use_cftime=use_cftime)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<xarray.plot.plot._PlotMethods at 0x7f83fd43c208>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Working with one dataset is better than many for learning\n",
    "\n",
    "path1 = '/srv/shared/deep_stac/data/forcings/2017/tmmx_2017.nc'\n",
    "#path2 = '/srv/shared/deep_stac/data/forcings/2017/tmmx_2017.nc'\n",
    "d1 = xr.open_dataset(path1)\n",
    "d1\n",
    "\n",
    "\n",
    "# Plot one time step of the data to visualize\n",
    "temp = d1.air_temperature.isel(day=0)\n",
    "#print(temp.isel(lat = 0 , lon = 0, day = 0).values)\n",
    "temp.plot\n",
    "#day[\"air_temperature\"][0].plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the spatial extent of the ASO dataset ... the datset is in UTM and forcing is in Lat/Long ... sad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-01-29\n",
      "2017-03-03\n",
      "2017-04-01\n",
      "2017-05-02\n",
      "2017-06-04\n",
      "2017-07-09\n",
      "2017-07-17\n",
      "2017-07-27\n",
      "2017-08-16\n",
      "datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# Bring in a list of the days or change the dates format? \n",
    "aso_path = '/srv/shared/deep_stac/data/snowdepth/netcdf/2017/*int.nc'\n",
    "fname=glob.glob(aso_path)\n",
    "aso_date = []\n",
    "\n",
    "for f in fname:\n",
    "    n = f.split('/')[-1]\n",
    "    t = n.split('_')[0]\n",
    "    t = pd.to_datetime(t).date()\n",
    "    aso_date.append(t)\n",
    "    aso_date = sorted(aso_date)\n",
    "for k in aso_date:\n",
    "    print(k)\n",
    "\n",
    "bins = pd.to_datetime(aso_date)\n",
    "print(bins.dtype)\n",
    "#da.sel(time=slice('2000-01-01', '2000-01-02')) \n",
    "#da.sel(time=slice('2000-01-01', '2000-01-02')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[254008.5 306712.5]\n",
      " [254008.5 306712.5]] [[4230328.5 4230328.5]\n",
      " [4179325.5 4179325.5]]\n",
      "-119.80870340068212 38.18732310852671\n",
      "-119.19368522229072 37.740828269550875\n",
      "<xarray.Dataset>\n",
      "Dimensions:          (crs: 1, day: 365, lat: 10, lon: 15)\n",
      "Coordinates:\n",
      "  * lon              (lon) float64 -119.8 -119.8 -119.7 ... -119.3 -119.3 -119.2\n",
      "  * lat              (lat) float64 38.15 38.11 38.07 38.03 ... 37.86 37.82 37.78\n",
      "  * day              (day) datetime64[ns] 2017-01-01 2017-01-02 ... 2017-12-31\n",
      "  * crs              (crs) uint16 3\n",
      "Data variables:\n",
      "    air_temperature  (day, lat, lon) float32 ...\n",
      "Attributes:\n",
      "    geospatial_bounds_crs:      EPSG:4326\n",
      "    Conventions:                CF-1.6\n",
      "    geospatial_bounds:          POLYGON((-124.7666666333333 49.40000000000000...\n",
      "    geospatial_lat_min:         25.066666666666666\n",
      "    geospatial_lat_max:         49.40000000000000\n",
      "    geospatial_lon_min:         -124.7666666333333\n",
      "    geospatial_lon_max:         -67.058333300000015\n",
      "    geospatial_lon_resolution:  0.041666666666666\n",
      "    geospatial_lat_resolution:  0.041666666666666\n",
      "    geospatial_lat_units:       decimal_degrees north\n",
      "    geospatial_lon_units:       decimal_degrees east\n",
      "    coordinate_system:          EPSG:4326\n",
      "    author:                     John Abatzoglou - University of Idaho, jabatz...\n",
      "    date:                       04 July 2019\n",
      "    note1:                      The projection information for this file is: ...\n",
      "    note2:                      Citation: Abatzoglou, J.T., 2013, Development...\n",
      "    note3:                      Data in slices after last_permanent_slice (1-...\n",
      "    note4:                      Data in slices after last_provisional_slice (...\n",
      "    note5:                      Days correspond approximately to calendar day...\n"
     ]
    }
   ],
   "source": [
    "# Get extent from ASO data: \n",
    "path_aso = '/srv/shared/deep_stac/data/snowdepth/netcdf/2017/20170129_SUPERsnow_depth_3m_int.nc'\n",
    "snow = xr.open_dataset(path_aso,decode_cf=True)\n",
    "\n",
    "# Get the extent of the flight line data: \n",
    "# NOTE: double check first and last are min and max \n",
    "xmin = snow['x'].values[0] #Easting to Lon\n",
    "xmax = snow['x'].values[-1] # Easting to Lon\n",
    "ymax = snow['y'].values[0] # Northing to Lat\n",
    "ymin = snow['y'].values[-1] #Northing to Lat\n",
    "\n",
    "\n",
    "bb = [xmin,xmax,ymin,ymax]\n",
    "crs_snow='32611'\n",
    "crs_temp ='4326'\n",
    "\n",
    "# Project the bounding box to Lat/Long \n",
    "x = np.array([xmin,xmax])\n",
    "y = np.array([ymin,ymax])\n",
    "xv,  yv = np.meshgrid(x, y)\n",
    "print(xv,yv)\n",
    "inProj = pyproj.Proj(init='epsg:32611')\n",
    "outProj = pyproj.Proj(init='epsg:4326')\n",
    "lon_min,lat_min = pyproj.transform(inProj,outProj,xmin,ymin)\n",
    "lon_max,lat_max = pyproj.transform(inProj,outProj,xmax,ymax)\n",
    "\n",
    "\n",
    "print(lon_min,lat_min)\n",
    "print(lon_max,lat_max)\n",
    "\n",
    "#Get the ESPG code for snow\n",
    "#print(snow.transverse_mercator.spatial_ref)\n",
    "\n",
    "# Subset the dataset based on the spatial extent of the ASO data \n",
    "sub_d1 = d1.sel(lon = slice(lon_min,lon_max), lat = slice(lat_min,lat_max))\n",
    "\n",
    "print(sub_d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-119.79125881719555 37.72818507361838\n"
     ]
    }
   ],
   "source": [
    "# How to reproject things! \n",
    "inProj = pyproj.Proj(init='epsg:32611')\n",
    "outProj = pyproj.Proj(init='epsg:4326')\n",
    "lon,lat = pyproj.transform(inProj,outProj,xmin,ymin)\n",
    "print(x2,y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bin the Forcings data based on the ASO flights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Interval('2017-01-29', '2017-03-03', closed='right'): [29,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  33,\n",
       "  34,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  40,\n",
       "  41,\n",
       "  42,\n",
       "  43,\n",
       "  44,\n",
       "  45,\n",
       "  46,\n",
       "  47,\n",
       "  48,\n",
       "  49,\n",
       "  50,\n",
       "  51,\n",
       "  52,\n",
       "  53,\n",
       "  54,\n",
       "  55,\n",
       "  56,\n",
       "  57,\n",
       "  58,\n",
       "  59,\n",
       "  60,\n",
       "  61],\n",
       " Interval('2017-03-03', '2017-04-01', closed='right'): [62,\n",
       "  63,\n",
       "  64,\n",
       "  65,\n",
       "  66,\n",
       "  67,\n",
       "  68,\n",
       "  69,\n",
       "  70,\n",
       "  71,\n",
       "  72,\n",
       "  73,\n",
       "  74,\n",
       "  75,\n",
       "  76,\n",
       "  77,\n",
       "  78,\n",
       "  79,\n",
       "  80,\n",
       "  81,\n",
       "  82,\n",
       "  83,\n",
       "  84,\n",
       "  85,\n",
       "  86,\n",
       "  87,\n",
       "  88,\n",
       "  89,\n",
       "  90],\n",
       " Interval('2017-04-01', '2017-05-02', closed='right'): [91,\n",
       "  92,\n",
       "  93,\n",
       "  94,\n",
       "  95,\n",
       "  96,\n",
       "  97,\n",
       "  98,\n",
       "  99,\n",
       "  100,\n",
       "  101,\n",
       "  102,\n",
       "  103,\n",
       "  104,\n",
       "  105,\n",
       "  106,\n",
       "  107,\n",
       "  108,\n",
       "  109,\n",
       "  110,\n",
       "  111,\n",
       "  112,\n",
       "  113,\n",
       "  114,\n",
       "  115,\n",
       "  116,\n",
       "  117,\n",
       "  118,\n",
       "  119,\n",
       "  120,\n",
       "  121],\n",
       " Interval('2017-05-02', '2017-06-04', closed='right'): [122,\n",
       "  123,\n",
       "  124,\n",
       "  125,\n",
       "  126,\n",
       "  127,\n",
       "  128,\n",
       "  129,\n",
       "  130,\n",
       "  131,\n",
       "  132,\n",
       "  133,\n",
       "  134,\n",
       "  135,\n",
       "  136,\n",
       "  137,\n",
       "  138,\n",
       "  139,\n",
       "  140,\n",
       "  141,\n",
       "  142,\n",
       "  143,\n",
       "  144,\n",
       "  145,\n",
       "  146,\n",
       "  147,\n",
       "  148,\n",
       "  149,\n",
       "  150,\n",
       "  151,\n",
       "  152,\n",
       "  153,\n",
       "  154],\n",
       " Interval('2017-06-04', '2017-07-09', closed='right'): [155,\n",
       "  156,\n",
       "  157,\n",
       "  158,\n",
       "  159,\n",
       "  160,\n",
       "  161,\n",
       "  162,\n",
       "  163,\n",
       "  164,\n",
       "  165,\n",
       "  166,\n",
       "  167,\n",
       "  168,\n",
       "  169,\n",
       "  170,\n",
       "  171,\n",
       "  172,\n",
       "  173,\n",
       "  174,\n",
       "  175,\n",
       "  176,\n",
       "  177,\n",
       "  178,\n",
       "  179,\n",
       "  180,\n",
       "  181,\n",
       "  182,\n",
       "  183,\n",
       "  184,\n",
       "  185,\n",
       "  186,\n",
       "  187,\n",
       "  188,\n",
       "  189],\n",
       " Interval('2017-07-09', '2017-07-17', closed='right'): [190,\n",
       "  191,\n",
       "  192,\n",
       "  193,\n",
       "  194,\n",
       "  195,\n",
       "  196,\n",
       "  197],\n",
       " Interval('2017-07-17', '2017-07-27', closed='right'): [198,\n",
       "  199,\n",
       "  200,\n",
       "  201,\n",
       "  202,\n",
       "  203,\n",
       "  204,\n",
       "  205,\n",
       "  206,\n",
       "  207],\n",
       " Interval('2017-07-27', '2017-08-16', closed='right'): [208,\n",
       "  209,\n",
       "  210,\n",
       "  211,\n",
       "  212,\n",
       "  213,\n",
       "  214,\n",
       "  215,\n",
       "  216,\n",
       "  217,\n",
       "  218,\n",
       "  219,\n",
       "  220,\n",
       "  221,\n",
       "  222,\n",
       "  223,\n",
       "  224,\n",
       "  225,\n",
       "  226,\n",
       "  227]}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try to bin the data by the date: \n",
    "sub_d1.groupby_bins('day', bins).groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to get Degree Days/ Subset by time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'dims'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-f2d606e0bbf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# make chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0msub_d1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby_bins\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'day'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreeze\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/site-packages/xarray/core/groupby.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, args, shortcut, **kwargs)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;31m# ignore shortcut if set (for now)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mds\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter_grouped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_combine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapplied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/site-packages/xarray/core/groupby.py\u001b[0m in \u001b[0;36m_combine\u001b[0;34m(self, applied)\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;34m\"\"\"Recombine the applied objects like the original.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0mapplied_example\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpeek_at\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m         \u001b[0mcoord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_concat_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied_example\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m         \u001b[0mcombined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mcombined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_reorder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/site-packages/xarray/core/groupby.py\u001b[0m in \u001b[0;36m_infer_concat_args\u001b[0;34m(self, applied_example)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_infer_concat_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapplied_example\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_group_dim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mapplied_example\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcoord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_group\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mpositions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_group_indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'dims'"
     ]
    }
   ],
   "source": [
    "#one_chunk = d1.sel(day=slice('2017-01-01',aso_date[0]),lon = slice(lon_min,lon_max), lat = slice(lat_min,lat_max))\n",
    "#temp_chunk = one_chunk.air_temperature\n",
    "\n",
    "\n",
    "\n",
    "# Run function to get a degree day for each pixel:\n",
    "# note temperature measurements are in K \n",
    "def freeze(x):\n",
    "    if x > 273.2:\n",
    "        x = 1\n",
    "    else:\n",
    "        x = 0\n",
    "\n",
    "#test = temp_chunk.apply(freeze)\n",
    "#for d in one_chunk.day:\n",
    "    #if air_temp > 273.2 K:\n",
    "        #cell = 1\n",
    "   # else: \n",
    "        #cell = 0 \n",
    "# make chunk \n",
    "\n",
    "sub_d1.groupby_bins('day', bins).apply(freeze)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
