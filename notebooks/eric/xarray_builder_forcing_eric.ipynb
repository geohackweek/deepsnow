{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to bring in ~~and downsample~~ the forcings and mash 'em together into a single xarray\n",
    "\n",
    "This code works off the xarray builder code first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try getting one of the files as a test before trying to pull them all in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to all of the data you want to load\n",
    "path = '/srv/shared/deep_stac/data/forcings/2017/*2017.nc'\n",
    "# Make the path a global varaible and make a list of the files \n",
    "flist=glob.glob(path)\n",
    "print(len(flist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All of the forcings data is contained within a text file called \"forcings_key.txt\"\n",
    "\n",
    "This file can be used to cycle through each netCDF input and stitch it together in the xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /home/jovyan/ghw2019_deepstac/notebooks/vermillion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and filter to the 'primary' forcings \n",
    "pth = '/home/jovyan/ghw2019_deepstac/notebooks/vermillion'\n",
    "fname = 'forcings_key.txt'\n",
    "\n",
    "i = 1\n",
    "key = np.genfromtxt(os.path.join(pth,fname), dtype='str', delimiter=',')\n",
    "\n",
    "len(key)\n",
    "\n",
    "pared = key[key[:,1]=='primary']\n",
    "\n",
    "# And the names of the files are in:\n",
    "pared[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a function for renaming (on the fly) input datasets with duplicate names\n",
    "(for example, inside the netCDF objects, both rmax and rmin were assigned a name of \"relative_humidity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.basename('/srv/shared/deep_stac/data/forcings/2014/pr_2014.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnm(ds):\n",
    "    filename = os.path.basename(ds.encoding['source'])\n",
    "    \n",
    "    if filename.startswith('tmmx'):\n",
    "        ds = ds.rename(dict(air_temperature='tmax'))\n",
    "    elif filename.startswith('tmmn'):\n",
    "        ds = ds.rename(dict(air_temperature='tmin'))\n",
    "    elif filename.startswith('rmax'):\n",
    "        ds = ds.rename(dict(relative_humidity='rmax'))        \n",
    "    elif filename.startswith('rmin'):\n",
    "        ds = ds.rename(dict(relative_humidity='rmin'))\n",
    "        \n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load up the data into xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the loation of the forcings data\n",
    "pth = '/srv/shared/deep_stac/data/forcings'\n",
    "yrs = ['2014', '2015', '2016', '2017']\n",
    "\n",
    "all_files = []\n",
    "for yr in yrs:\n",
    "    for itm in pared[:,0]:\n",
    "        # For reasons unbeknownst to the user, these two datasets aren't coming in.\n",
    "        #if any(x in itm for x in ['rmin']):   #,'tmmx'\n",
    "        #    continue\n",
    "        all_files.append(os.path.join(pth,yr,itm.split('_')[0] + '_' + yr + '.nc'))\n",
    "\n",
    "# merge all forcings data into a single xarray in one go.\n",
    "ds = xr.open_mfdataset(all_files, preprocess=rnm,  combine='by_coords')  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just for grins, here is how to visualize one of the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with one dataset is better than many for learning\n",
    "\n",
    "path1 = '/srv/shared/deep_stac/data/forcings/2017/tmmx_2017.nc'\n",
    "d1 = xr.open_dataset(path1)\n",
    "# d1['day'][0]['air_temperature'].plot\n",
    "d1\n",
    "temp=d1.air_temperature\n",
    "temp.isel(day=0).plot()\n",
    "#day[\"air_temperature\"][0].plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And here is the guts of the xarray (should one be interested):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:notebook] *",
   "language": "python",
   "name": "conda-env-notebook-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
